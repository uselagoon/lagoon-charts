# Default values for lagoon-logging.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

nameOverride: ""
fullnameOverride: ""

# this default image tag is set for all services and can be overridden
# on the service level, if not set it falls back to chart appVersion.
imageTag: ""

logsDispatcher:

  name: logs-dispatcher

  replicaCount: 3

  image:
    repository: uselagoon/logs-dispatcher
    pullPolicy: IfNotPresent
    # Overrides the image tag whose default is "latest".
    tag: "v3.3.0"

  serviceAccount:
    # Specifies whether a service account should be created
    create: true
    # Annotations to add to the service account
    annotations: {}
    # The name of the service account to use.
    # If not set and create is true, a name is generated using the fullname
    # template
    # If this value is set, the serviceAccount named must have clusterrole
    # view.
    name: ""

  podAnnotations: {}

  podSecurityContext: {}
    # fsGroup: 2000

  securityContext: {}
    # capabilities:
    #   drop:
    #   - ALL
    # readOnlyRootFilesystem: true
    # runAsNonRoot: true
    # runAsUser: 1000

  resources: {}
    # If you want to specify resources, uncomment the following lines, adjust
    # them as necessary, and remove the curly braces after 'resources:'.
    # limits:
    #   cpu: 100m
    #   memory: 128Mi
    # requests:
    #   cpu: 100m
    #   memory: 128Mi

  nodeSelector: {}

  tolerations: []

  affinity: {}

  serviceMonitor:
    enabled: true

  buffer:
    size: 16Gi
    storageClassName: ""

openshiftHaproxyLogsCollector:

  enabled: false

  name: openshift-haproxy-logs-collector

  replicaCount: 3

  image:
    repository: fluent/fluent-bit
    pullPolicy: IfNotPresent
    # Overrides the image tag whose default is the chart version.
    tag: "1.6-debug"

  podAnnotations: {}

  podSecurityContext:
    fsGroup: 0

  securityContext: {}

  resources: {}
    # If you want to specify resources, uncomment the following lines, adjust
    # them as necessary, and remove the curly braces after 'resources:'.
    # limits:
    #   cpu: 100m
    #   memory: 128Mi
    # requests:
    #   cpu: 100m
    #   memory: 128Mi

  nodeSelector: {}

  tolerations: []

  affinity: {}

  serviceMonitor:
    enabled: true

cdnLogsCollector:

  enabled: false

  name: cdn-logs-collector

  replicaCount: 3

  image:
    repository: uselagoon/logs-dispatcher
    pullPolicy: IfNotPresent
    # Overrides the image tag whose default is "latest".
    tag: "v3.3.0"

  podAnnotations: {}

  podSecurityContext: {}

  securityContext: {}

  resources: {}

  nodeSelector: {}

  tolerations: []

  affinity: {}

  serviceMonitor:
    enabled: true

  buffer:
    size: 8Gi
    storageClassName: ""

  serviceType: LoadBalancer

  # TLS configuration is required
  # These should be server certificates, and the CDN should be configured to
  # present client certificates with the same trust root.
  tls:
    caCert: ""
    serverCert: ""
    serverKey: ""

# Don't collect logs from these namespaces.
# Comment out this field to collect from all namespaces.
excludeNamespaces:
# k8s
- cattle-prometheus
- cattle-system
- dbaas-operator
- default
- kube-cleanup-operator
- kube-node-lease
- kube-public
- kube-system
- metrics-server
- syn
- syn-backup
- syn-cert-manager
- syn-cluster-autoscaler
- syn-efs-provisioner
- syn-resource-locker
- syn-synsights
# openshift
- acme-controller
- appuio-baas-operator
- appuio-dnsmonitor
- appuio-espejo
- appuio-infra
- appuio-monitoring
- appuio-pruner
- appuio-tiller
- dioscuri-controller
- kube-service-catalog
- management-infra
- monitoring-infra
- openshift
- openshift-ansible-service-broker
- openshift-console
- openshift-infra
- openshift-logging
- openshift-metrics-server
- openshift-monitoring
- openshift-node
- openshift-sdn
- openshift-web-console
- tiller

# namespaces to exclude in addition to the default list above
extraExcludeNamespaces: []
# - extra-namespace-0
# - extra-namespace-1

# Configure the cluster output buffer.
# This may require tweaking to handle high volumes of logs.
clusterOutputBuffer:
  flush_thread_count: 32
  # create chunks every 10 secs, therefore possible to flush every 10sec
  timekey: 10s
  # don't wait additional time for the timekey to be filled
  timekey_wait: 0s
  timekey_use_utc: true
  # don't sleep if there is more data to flush
  flush_thread_burst_interval: '0'
  # flush immediate, the forward target is in the same cluster so this should
  # be able to handle it
  flush_mode: immediate
  # limit exponential backoff period
  retry_max_interval: 30s

# chart dependency on logging-operator
logging-operator:
  enabled: true
  monitoring:
    serviceMonitor:
      enabled: true
      additionalLabels:
        monitoring.lagoon.sh/monitorMe: 'true'

# lagoon logs collection disabled by default. see below for instructions on
# enabling this.
lagoonLogs:
  enabled: false

forward:
  verifyConnectionAtStartup: true
  tlsVerifyHostname: true

# Some cluster services use ephemeral namespaces that cause many indices to be
# created in elasticsearch. Setting this to true will cause these service logs
# to be consolidated into a single index per service.
consolidateServiceIndices: false

# Optionally default logs forwarding may be disabled if e.g. all logs are being
# sent to a third-party service and not to a central elasticsearch.
enableDefaultForwarding: true

# Set how many fluentd pods should be running
fluentdReplicaCount: 3

# Add tolerations to the fluentbit daemonset so that it runs on e.g.
# load-balancer nodes.
fluentbitTolerations:
- effect: NoSchedule
  key: lagoon.sh/lb
  operator: Exists
- effect: NoSchedule
  key: lagoon.sh/spot
  operator: Exists

# Expose metrics of the Logging Operator's fluentbit daemonset and fluentd
# statefulset via a Prometheus Operator compatible ServiceMonitor object.
#
# The fluentd serviceMonitor is disabled by default until a use-case is found.
#
# See here for full documentation of this field:
# https://kube-logging.dev/docs/operation/logging-operator-monitoring/#metrics-variables
fluentbitMetrics:
  serviceMonitor: true
# fluentdMetrics:
#   serviceMonitor: true

# This chart assumes the container runtime is containerd, which puts the log
# message in the `message` field of the log record.
#
# If the container runtime is docker, then the log message is in the `log`
# field instead. Set value to `docker` to enable docker compatible log parsing.
# Currently the only valid values are `docker` or an empty string (the
# default).
containerRuntime: ""

# The ingress-nginx controller logs are extremely verbose, but sometimes
# useful. By default we discard these logs. But setting this value to true will
# capture the logs in an index with the prefix `router-logs-ingress-nginx_`.
keepIngressNginxController: false

# The values below must be supplied during installation.
# Certificates should be provided in PEM format, and are generated as described
# in the README for the lagoon-logs-concentrator chart.
# Sample data shown below.

# tls:
#   caCert: |
#     -----BEGIN CERTIFICATE-----
#     ...
#     -----END CERTIFICATE-----
#   clientCert: |
#     -----BEGIN CERTIFICATE-----
#     ...
#     -----END CERTIFICATE-----
#   clientKey: |
#     -----BEGIN EC PRIVATE KEY-----
#     ...
#     -----END EC PRIVATE KEY-----
#
# forward:
#   username: "example1"
#   password: "securepass"
#   host: "203.0.113.9"
#   # hostName is optional - it is used for TLS verification for when host is an
#   # IP address.
#   # NOTE: if host is _not_ an IP address and it is presents a certificate
#   # without that hostname, you'll also need to set tlsVerifyHostname to
#   # false. The hostName field does _not_ override the host field for TLS
#   # verification when host is not an IP address.
#   hostName: "logs.server.example.com"
#   # tlsVerifyHostname: false
#   # hostPort is optional, default 24224
#   hostPort: "24224"
#   selfHostname: "logs-dispatcher.example1.lagoon.example.com"
#   sharedKey: "supersecurekey"
#
# clusterName: "example1"

# Optional lagoon logs configuration. This should be enabled on a full lagoon
# install, but not in a lagoon-remote install. If enabled, the rabbitMQ* values
# are required.
#
# lagoonLogs:
#   enabled: true
#   rabbitMQHost: secureuser
#   rabbitMQUser: secureuser
#   rabbitMQPassword: secureuser

# Optional namespace selection. Logs will _only_ be collected from these
# namespaces. You probably don't want to configure this, except for debugging.
#
# selectNamespaces:
# - drupal-example

# Optional log export configuration
#
# exportLogs:
#   s3.conf: |
#     <store ignore_error>
#       @type s3
#       ...
#     </store>
#   cloudwatch.conf: |
#     <store ignore_error>
#       @type cloudwatch_logs
#       ...
#     </store>

# Optional extra fluentd configuration.
#
# This adds a <label @EXTRACONF></label> section to the fluentd configuration
# containing the given configuration, and copies logs to that label after all
# the Lagoon-specific munging. It uses the pattern described here:
# https://docs.fluentd.org/configuration/routing-examples#re-route-event-to-other-label
#
# extraConf: |-
#   <match lagoon.*.router.**>
#     @type sumologic
#     ...
#   </match>
#   <match lagoon.*.container>
#     @type sumologic
#     ...
#   </match>
#   <match lagoon.*.application>
#     @type sumologic
#     ...
#   </match>

# Openshift only!
#
# fluentbitPrivileged: true

# Optional post-processing of router logs.
#
# This value allows you to insert a snippet of fluentd configuration into the
# router logs processing pipeline directly before output. This can be used to
# perform additional custom parsing of router logs. Please only use
# <filter>...</filter>, and do not retag records in this field to avoid
# breaking the log pipeline.
#
# This example will:
# * set is_facet_page to true if the request_query field contains "f[0]".
# * set is_search_page to true if the request_query field contains "search_api".
# * set is_search_bot to true if the http_user_agent field is FooBot or BarBot.
#
# routerLogsPostProcess: |-
#   <filter lagoon.*.router.nginx>
#     @type record_modifier
#     <record>
#       is_facet_page ${!record["request_query"]&.match(/f\[0\]/).nil?}
#     </record>
#     <record>
#       is_search_page ${!record["request_query"]&.match(/search_api/).nil?}
#     </record>
#     <record>
#       is_search_bot ${!record["http_user_agent"]&.match(/\A(FooBot|BarBot)\z/).nil?}
#     </record>
#   </filter>

# Install test fixtures into the cluster.
# This should _only_ be used in a test cluster, because it creates namespaces for testing.
# Do not set testFixtures.create=true in a production environment.
testFixtures:
  create: false
